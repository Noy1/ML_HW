{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Advance_model-Harel&Noy.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Kgk7_mobYK-G"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import cross_val_score\n","from sklearn.linear_model import LinearRegression, Ridge\n","from sklearn.model_selection import GridSearchCV\n","from xgboost import XGBRegressor\n","from sklearn.ensemble import GradientBoostingRegressor\n","from sklearn.ensemble import RandomForestRegressor\n","from geopy.geocoders import Nominatim\n","from geopy.distance import geodesic\n","from mlxtend.regressor import StackingCVRegressor"]},{"cell_type":"markdown","source":["### Main functions"],"metadata":{"id":"lsMD9dw_YWHf"}},{"cell_type":"code","source":["def create_normalize_feature(df, feature, ordered_categories):\n","    weight = 1/(len(ordered_categories)-1)\n","    for i in range(len(ordered_categories)):\n","        df[feature].replace(ordered_categories[i], round(weight*(i),3), \n","                            inplace= True)"],"metadata":{"id":"n0QaTuulYUIc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def merge_categories_by_threshold(df, label, new_col_name, threshold):\n","    value_counts_label = df.value_counts(df[label])\n","    mask = (value_counts_label / value_counts_label.sum() * 100).lt(threshold)\n","    new_df = df.assign(new_label = np.where(df[label].isin(\n","                          value_counts_label[mask].index), 'Other', df[label]))\n","    new_df.rename(columns={'new_label': new_col_name}, inplace=True)\n","    new_df.drop(columns = label, inplace=True)\n","    return new_df"],"metadata":{"id":"cag90QC_YZez"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Preprocess"],"metadata":{"id":"FFcB_eGRYcNu"}},{"cell_type":"code","source":["def garage_features(df):\n","    df.loc[df['GarageType'].notnull(), 'GarageType'] = 1\n","    df.GarageType.fillna(0, inplace=True)\n","    df.rename(columns={'GarageType':'Garage'}, inplace=True)\n","\n","def lot_frontage_feature(df):\n","    df.LotFrontage.fillna(df.LotFrontage.mean(), inplace=True)\n","\n","def veneer_features(df):\n","    df.loc[df['MasVnrType'].notnull(), 'MasVnrType'] = 1\n","    df.MasVnrType.fillna(0, inplace=True)\n","    df['MasVnrType'][df.MasVnrArea == 0] = 0\n","    df.rename(columns={'MasVnrType':'MasVnr'}, inplace=True)\n","    df.MasVnrArea.fillna(0, inplace=True)\n","\n","def fill_nan_categories(df):\n","    df.Functional.fillna('Typ', inplace=True)\n","    df.SaleType.fillna('WD', inplace=True)\n","    df.Exterior1st.fillna('VinylSd', inplace=True)\n","    df.Exterior2nd.fillna('VinylSd', inplace=True)\n","\n","def electrical_feature(df):\n","    df.Electrical.fillna('SBrkr', inplace=True)\n","    ordered_categories = ['Mix', 'FuseP', 'FuseF', 'FuseA', 'SBrkr']\n","    create_normalize_feature(df, 'Electrical', ordered_categories)\n","\n","def exterior_feature(df):\n","    df.Exterior1st.fillna('None', inplace=True)\n","    df.Exterior2nd.fillna('None', inplace=True)\n","    df['Exterior_combined'] = df.apply(lambda x: x.Exterior1st + '_' + \n","                                             x.Exterior2nd, axis=1)\n","    df.drop(columns=['Exterior1st', 'Exterior2nd'], inplace=True)"],"metadata":{"id":"KRObXK_7YbTD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def fillna_continues_features(df, continues_features):\n","    for feature in continues_features:\n","        df.loc[np.isnan(df[feature]), feature] = 0\n","\n","def encoding_ordinal_categories(df, categorial_dict):\n","    for feature, ordered_categories in categorial_dict.items():\n","        create_normalize_feature(df, feature, ordered_categories)\n","        df.loc[np.isnan(df[feature]), feature] = 0\n","\n","def special_categories_treatment(df):\n","    fill_nan_categories(df)\n","    garage_features(df)\n","    lot_frontage_feature(df)\n","    veneer_features(df)\n","    electrical_feature(df)\n","    exterior_feature(df)\n","\n","def remove_categorical_features(df, columns):\n","    df_columns = df.columns\n","    for feature in columns:\n","        if feature in df_columns:\n","            df.drop(columns = [feature], inplace=True)\n","\n","def merge_categories(df, merged_features_dict, threshold):\n","    for feature, new_feature in merged_features_dict.items():\n","        df = merge_categories_by_threshold(df, feature, new_feature, threshold)\n","    return df\n","\n","def unicode_features(df, unicode_dict):\n","    for feature, new_unicode_dict in unicode_dict.items():\n","        df[feature] = df[feature].map(new_unicode_dict) \n","\n","def create_onehot_features(df, onehot_columns):\n","  data = pd.get_dummies(df, prefix=onehot_columns, columns=onehot_columns)\n","  return data\n"],"metadata":{"id":"O4fOU91zYgTa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["continues_features = ['GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', \n","                      'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', \n","                      'GarageYrBlt']\n","\n","category_ordered = {'PoolQC': ['Na', 'Fa', 'TA', 'Gd', 'Ex'],\n","                    'Fence': ['Na', 'MnWw', 'GdWo', 'MnPrv', 'GdPrv'],\n","                    'FireplaceQu': ['Na', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n","                    'GarageFinish': ['Na', 'Unf', 'RFn', 'Fin'],\n","                    'GarageQual': ['Na', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n","                    'GarageCond': ['Na', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n","                    'BsmtQual': ['NA','Po', 'Fa', 'TA', 'Gd', 'Ex'],\n","                    'BsmtCond': ['NA','Po', 'Fa', 'TA', 'Gd', 'Ex'],\n","                    'BsmtExposure': ['NA','No', 'Mn', 'Av', 'Gd'],\n","                    'BsmtFinType1': ['NA','Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],\n","                    'BsmtFinType2': ['NA','Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],\n","                    'HeatingQC': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n","                    'KitchenQual': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n","                    'ExterQual': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n","                    'ExterCond': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n","                    'LotShape': ['IR3', 'IR2', 'IR1', 'Reg'],\n","                    'LandContour': ['Low', 'HLS', 'Bnk', 'Lvl'],\n","                    'LandSlope': ['Sev', 'Mod', 'Gtl'],\n","                    'BldgType': ['Twnhs', 'TwnhsE', 'Duplex', '2fmCon', '1Fam'],\n","                    'PavedDrive': ['N', 'P', 'Y'],\n","                    'Utilities': ['ELO', 'NoSeWa', 'NoSewr', 'AllPub']}\n","            \n","merged_features_dict = {'RoofMatl': 'RoofMatl_CompShg', \n","                        'Heating': 'Heating_GasA',\n","                        'Functional': 'Functional_typical',\n","                        'Foundation': 'Foundation_merged',\n","                        'SaleCondition': 'SaleCondition_merged',\n","                        'Exterior_combined': 'Exterior_combined_merged',\n","                        'Condition2': 'Condition2_norm'}\n","                                            \n","unicode_features_dict = {'Street': {'Grvl': 1, 'Pave': 0},\n","                          'CentralAir': {'Y': 1, 'N': 0},\n","                          'RoofMatl_CompShg': {'CompShg': 1, 'Other': 0},\n","                          'Heating_GasA': {'GasA': 1, 'Other': 0},\n","                          'Functional_typical': {'Typ': 1, 'Other': 0},\n","                          'Condition2_norm': {'Norm': 1, 'Other': 0}}\n","\n","onehot_columns = ['SaleCondition_merged']\n","\n","remove_features_list = ['LotConfig', 'Condition1', 'MoSold', 'MiscVal',\n","                        'SaleCondition_merged_Other']"],"metadata":{"id":"QR1q0whMYm_K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocessing(df, continues_features, category_ordered, remove_features_list, \n","                  merged_features_dict, unicode_features_dict, onehot_columns):\n","    fillna_continues_features(df, continues_features)\n","    encoding_ordinal_categories(df, category_ordered)\n","    special_categories_treatment(df)\n","    df = merge_categories(df, merged_features_dict, 5)\n","    unicode_features(df, unicode_features_dict)\n","    data = create_onehot_features(df, onehot_columns)\n","    remove_categorical_features(data, remove_features_list)\n","    data.set_index(['Id'], inplace=True)\n","    return data"],"metadata":{"id":"Z4WDVE_aYk5K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Changes in preprocess\n","\n","We decided to find a better way to preprocess those features:\n","- Neighborhood \n","- HouseStyle \n","- RoofStyle \n","- SaleType \n","- Foundation\n","- Exterior\n","\n","On the basic model we used \"onehot encoding\", it's created a massive number of new features because the features above has more then 2 categories. Now we decided to use \"target encoding\" which replaces a categorical value with the average value of the output (ie. target) for that value of the feature.\n"," * On 2 features: Foundation, Exterior we decided first to merge categories that are under 5% of the data because we saw from our EDA that they are not contibute a lot.\n"," * On the other features we decided not to merge categories under 5% because we saw that thier correlation with Y is pretty high.\n","\n","We saw that it's improve our results so we add it to our preprocess and remove the original feature.\n","\n","* The only category that we decided to use as onehot feature is \"SaleCondition\" that was better as onehot then as avg values. we merge the catefories that are less then 5% from our data and remove 1 onehot column because to represent all the categories is inufe to use num_categories-1 of the columns."],"metadata":{"id":"UQNlD7jkYvis"}},{"cell_type":"code","source":["def target_encoding(df, features):\n","  group_by_avg = {}\n","  df_columns = df.columns\n","  for feature in features:\n","    feature_means = df.groupby(feature)['SalePrice'].mean()\n","    group_by_avg[feature] = feature_means\n","    df[feature + \"_by_avg\"] = df[feature].map(feature_means)\n","    df.drop(columns = [feature], inplace=True)\n","  return group_by_avg"],"metadata":{"id":"r-L-1LmSEOwc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test = pd.read_csv('test.csv')\n","train = pd.read_csv('train.csv')\n","\n","processed_train = preprocessing(train, continues_features, category_ordered, \n","                                remove_features_list, merged_features_dict, \n","                                unicode_features_dict, onehot_columns)\n","\n","features = ['Neighborhood', 'HouseStyle', 'RoofStyle', 'SaleType', \n","            'Foundation_merged', 'Exterior_combined_merged']\n","                    \n","target_encoding(processed_train, features)\n","\n","y = processed_train['SalePrice']\n","y = np.log1p(y)\n","X = processed_train.copy().loc[:, processed_train.columns != 'SalePrice']"],"metadata":{"id":"hXjPQc9SjF-J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#example:\n","processed_train['RoofStyle_by_avg'].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-NtNqm-faGCJ","executionInfo":{"status":"ok","timestamp":1643486240124,"user_tz":-120,"elapsed":12,"user":{"displayName":"Harel Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwJ3OCVyJY65jKLzMqY5SMvTdvTdICLKM2HMDchA=s64","userId":"02907729866407537485"}},"outputId":"353dcd63-5ee8-467c-9812-aa778c0a4d72"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["171483.956179    1141\n","218876.933566     286\n","194690.000000      13\n","148909.090909      11\n","180568.428571       7\n","225000.000000       2\n","Name: RoofStyle_by_avg, dtype: int64"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["#basic model score\n","clf = LinearRegression()\n","scores = cross_val_score(clf, X, y, cv=5)\n","print(scores)\n","print(np.mean(scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vUwMi6Q-bwRy","executionInfo":{"status":"ok","timestamp":1643486240124,"user_tz":-120,"elapsed":8,"user":{"displayName":"Harel Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwJ3OCVyJY65jKLzMqY5SMvTdvTdICLKM2HMDchA=s64","userId":"02907729866407537485"}},"outputId":"eb20c451-7cf9-4d3b-b2bf-de9ff94de291"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.89733983 0.87580673 0.90354124 0.89418906 0.78431095]\n","0.8710375626886726\n"]}]},{"cell_type":"markdown","source":["# Feature Engineering"],"metadata":{"id":"UUMclNVdrdIb"}},{"cell_type":"markdown","source":["## New feature- distance from university\n","\n","We found out thata the neighborhoods locations are around an university, so we decided to define a new feature that will represent the distance from the university because we saw it's influence on house price."],"metadata":{"id":"D0awl5-ZXsPU"}},{"cell_type":"code","source":["neighborhoods_dict = {'Blmngtn': 'Bloomington Heights, Ames, IA',\n","                      'Blueste': 'Bluestem, Ames, IA',\n","                      'BrDale': 'Briardale, Ames, IA',\n","                      'BrkSide': 'Brookside, Ames, IA',\n","                      'ClearCr': 'Clear Creek, Ames, IA',\n","                      'CollgCr': 'College Creek, Ames, IA',\n","                      'Crawfor': 'Crawford, Ames, IA',\n","                      'Edwards': 'Edwards, Ames, IA',\n","                      'Gilbert': 'Gilbert, IA',\n","                      'IDOTRR': 'Iowa Department of Transportation, Ames, IA',\n","                      'MeadowV': 'Meadow Place, Ames, IA',\n","                      'Mitchel': 'Mitchell, Ames, IA',\n","                      'NAmes': 'North Ames, Ames, IA',\n","                      'NoRidge': 'Northridge, Ames, IA',\n","                      'NPkVill': 'parkview, Ames, IA',\n","                      'NridgHt': 'Northridge Heights, Ames, IA',\n","                      'NWAmes': 'Northwest Ames, Ames, IA',\n","                      'OldTown': 'Old Town, Ames, IA',\n","                      'SWISU': 'Iowa State University, Ames, IA',\n","                      'Sawyer': 'Garfield Cir, Ames, IA',\n","                      'SawyerW': 'Illinois Ave, Ames, IA',\n","                      'Somerst': 'Somerset, Ames, IA',\n","                      'StoneBr': 'Stone Brooke, Ames, IA',\n","                      'Timber': 'Timberland, Ames, IA',\n","                      'Veenker': 'Veenker, Ames, IA'}"],"metadata":{"id":"YL8kr_XKXxa5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def find_geolocation(neighborhoods_dict):\n","  location = {}\n","  geolocator = Nominatim(user_agent='my_request')\n","  for key in neighborhoods_dict:\n","      loc = geolocator.geocode(neighborhoods_dict[key])\n","      location[key] = (loc.latitude, loc.longitude)\n","  return location\n","\n","def find_distance_from_university():\n","  neighborhood_locations = find_geolocation(neighborhoods_dict)\n","  university = neighborhood_locations['SWISU']\n","  distance_from_uni = {}\n","  for key in neighborhood_locations:\n","      distance_from_uni[key] = geodesic(neighborhood_locations[key], university).km\n","  return distance_from_uni"],"metadata":{"id":"EAhrN_EcX4rI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test = pd.read_csv('test.csv')\n","train = pd.read_csv('train.csv')\n","\n","distance_from_uni = find_distance_from_university()\n","train['dist_from_uni'] = train.Neighborhood.map(distance_from_uni)\n","\n","processed_train = preprocessing(train, continues_features, category_ordered, \n","                                remove_features_list, merged_features_dict, \n","                                unicode_features_dict, onehot_columns)\n","\n","target_encoding(processed_train, features)\n","\n","y = processed_train['SalePrice']\n","y = np.log1p(y)\n","X = processed_train.copy().loc[:, processed_train.columns != 'SalePrice']"],"metadata":{"id":"JpE_k7bfZRuz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#basic model score\n","clf = LinearRegression()\n","scores = cross_val_score(clf, X, y, cv=5)\n","print(scores)\n","print(np.mean(scores))"],"metadata":{"id":"uqSiOQWZaCa6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643486252939,"user_tz":-120,"elapsed":14,"user":{"displayName":"Harel Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwJ3OCVyJY65jKLzMqY5SMvTdvTdICLKM2HMDchA=s64","userId":"02907729866407537485"}},"outputId":"6abd1cb9-90d8-4803-d678-cca83cd35ac9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.89698064 0.87699995 0.90219945 0.89508086 0.78523665]\n","0.8712995084305417\n"]}]},{"cell_type":"markdown","source":["## Features Interaction\n","We want to emphasize the differences of components in houses, to do that we are using features interactions."],"metadata":{"id":"6co77tixFAy9"}},{"cell_type":"markdown","source":["* **Quality * Size**- when we combine the quality with the area we might get some differences between components because the combination creates new \"score\" that combine the 2 parameters and helps to evaluation."],"metadata":{"id":"N4VDiBA5FH-c"}},{"cell_type":"code","source":["X_copy = X.copy()\n","\n","#quality*size basement finish1\n","X_copy.loc[X_copy['BsmtFinSF1'] > 0, \"BsmtFinSF1\"] = (X_copy[\"BsmtFinSF1\"] * \n","                                                      X_copy[\"BsmtFinType1\"])\n","#quality*size basement finish2 \n","X_copy.loc[X_copy['BsmtFinSF2'] > 0, \"BsmtFinSF2\"] = (X_copy[\"BsmtFinSF2\"] *\n","                                                      X_copy[\"BsmtFinType2\"])\n","#basement quality*size \n","X_copy.loc[X_copy['TotalBsmtSF'] > 0, \"TotalBsmtSF\"] = (X_copy[\"TotalBsmtSF\"] *\n","                                                        X_copy[\"BsmtCond\"])\n","\n","#vaneer quality*size \n","X_copy.loc[X_copy['MasVnrArea'] > 0, \"MasVnrArea\"] = (X_copy[\"MasVnrArea\"] *\n","                                                  X_copy[\"MasVnr\"])\n","\n","#Exterior quality*size\n","X_copy.loc[X_copy['ExterQual'] > 0, \"ExterQual\"] = (X_copy[\"ExterQual\"] *\n","                                                   X_copy[\"ExterCond\"])\n","\n","#Garage quality*size\n","X_copy.loc[X_copy['GarageArea'] > 0, \"GarageArea\"] = (X_copy[\"GarageArea\"] *\n","                                                   X_copy[\"GarageCond\"])\n","\n","#Fireplace quality*size\n","X_copy.loc[X_copy['Fireplaces'] > 0, \"Fireplaces\"] = (X_copy[\"Fireplaces\"] *\n","                                                   X_copy[\"FireplaceQu\"])\n","\n","X_copy.rename(columns={'TotalBsmtSF':'BsmtSizeQual', 'ExterQual': 'Exterior'}, \n","              inplace=True)\n","\n","X_copy.drop(columns = [\"BsmtFinType1\", \"BsmtFinType2\", \"BsmtCond\",\n","                       \"MasVnr\", 'ExterCond', \"FireplaceQu\"], inplace=True)"],"metadata":{"id":"B95zqMvsE_r4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* **Percent of total size**- when we show the percent of parameter from the hole we can see the bigger picture and compare propertly between diffrent records."],"metadata":{"id":"uotXvndiIIFr"}},{"cell_type":"code","source":["#percent finish1 basement\n","X_copy.loc[X_copy['BsmtSizeQual'] > 0, \"BsmtFinSF1\"] = (X_copy[\"BsmtFinSF1\"]/\n","                                                        X_copy[\"BsmtSizeQual\"])\n","#percent finish2 basement \n","X_copy.loc[X_copy['BsmtSizeQual'] > 0, \"BsmtFinSF2\"] = (X_copy[\"BsmtFinSF2\"]/\n","                                                        X_copy[\"BsmtSizeQual\"])\n","#percent unfinished basement \n","X_copy.loc[X_copy['BsmtSizeQual'] > 0, \"BsmtUnfSF\"] = (X_copy[\"BsmtUnfSF\"]/\n","                                                       X_copy[\"BsmtSizeQual\"])\n","\n","#percent bedrooms above grade from all rooms above grade  \n","X_copy.loc[X_copy['TotRmsAbvGrd'] > 0, \"BedroomAbvGr\"] = (X_copy[\"BedroomAbvGr\"]/\n","                                                       X_copy[\"TotRmsAbvGrd\"])\n","\n","#percent kitchens above grade from all rooms above grade  \n","X_copy.loc[X_copy['TotRmsAbvGrd'] > 0, \"KitchenAbvGr\"] = (X_copy[\"KitchenAbvGr\"]/\n","                                                       X_copy[\"TotRmsAbvGrd\"])"],"metadata":{"id":"s4zDMf9-H-Ue"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Proportion by feature**- we tried to score the half bath in a diffrent way from the baths to united them to one feature that show the number of baths but more evaluation full baths then half baths "],"metadata":{"id":"-e23wEhgg_5r"}},{"cell_type":"code","source":["X_copy[\"BsmtBaths\"] = (X_copy[\"BsmtHalfBath\"]*0.5) + X_copy[\"BsmtFullBath\"]\n","X_copy.drop(columns = [\"BsmtHalfBath\", \"BsmtFullBath\"], inplace=True)"],"metadata":{"id":"v0dN5SJzhAVX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#basic model score\n","clf = LinearRegression()\n","scores = cross_val_score(clf, X_copy, y, cv=5)\n","print(scores)\n","print(np.mean(scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tZNf1EBdI2Eh","executionInfo":{"status":"ok","timestamp":1643486253652,"user_tz":-120,"elapsed":7,"user":{"displayName":"Harel Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwJ3OCVyJY65jKLzMqY5SMvTdvTdICLKM2HMDchA=s64","userId":"02907729866407537485"}},"outputId":"a030d5a6-07b9-4d57-f845-d86b949e1fc3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8963146  0.88179761 0.90460539 0.90038839 0.79956168]\n","0.8765335333213002\n"]}]},{"cell_type":"markdown","source":["## Normalization"],"metadata":{"id":"9xM-zqo-rjLq"}},{"cell_type":"markdown","source":["### Years features\n","\n","We want to normalize the data but first we need to deal with the various years categories. We do it by measuring the difference from the maximal year so we get a measure of how recent is this year. Then we can apply MinMax normalization as usual.\n"],"metadata":{"id":"Y5Z4MuW6ro0Q"}},{"cell_type":"code","source":["years_features = ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'YrSold']\n","max_year = X_copy[years_features].max()\n","X_copy[years_features] = max_year - X_copy[years_features]"],"metadata":{"id":"qkaejAOXrcXr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### MinMax normalization"],"metadata":{"id":"jpFBTCWGrwCP"}},{"cell_type":"code","source":["scaler = MinMaxScaler()\n","features_to_normalize = ['OverallQual', 'OverallCond', \"LotArea\", \"LotFrontage\",\n","                         'YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'YrSold', \n","                         'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', \n","                         '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', \n","                         'GarageArea', 'WoodDeckSF', 'OpenPorchSF', \n","                         'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n","                         'Neighborhood_by_avg', 'HouseStyle_by_avg', \n","                         'BsmtSizeQual','RoofStyle_by_avg', 'SaleType_by_avg',\n","                         'Foundation_merged_by_avg', 'Exterior_combined_merged_by_avg']\n","\n","scaler.fit(X_copy[features_to_normalize])\n","X_copy[features_to_normalize] = scaler.transform(X_copy[features_to_normalize])"],"metadata":{"id":"Dr17kJ_VrzOC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#basic model score\n","clf = LinearRegression()\n","scores = cross_val_score(clf, X_copy, y, cv=5)\n","print(scores)\n","print(np.mean(scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aOaRQ_Olti2R","executionInfo":{"status":"ok","timestamp":1643486253653,"user_tz":-120,"elapsed":7,"user":{"displayName":"Harel Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwJ3OCVyJY65jKLzMqY5SMvTdvTdICLKM2HMDchA=s64","userId":"02907729866407537485"}},"outputId":"6bcc4232-8187-4ae3-fb90-f2e2723e2389"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8963146  0.88179761 0.90460539 0.90038839 0.79956168]\n","0.8765335333218779\n"]}]},{"cell_type":"markdown","source":["# Feature engineering A-Z"],"metadata":{"id":"5ya81Si1JQwl"}},{"cell_type":"code","source":["def find_geolocation(neighborhoods_dict):\n","  location = {}\n","  geolocator = Nominatim(user_agent='my_request')\n","  for key in neighborhoods_dict:\n","      loc = geolocator.geocode(neighborhoods_dict[key])\n","      location[key] = (loc.latitude, loc.longitude)\n","  return location\n","\n","def find_distance_from_university(neighborhoods_dict):\n","  neighborhood_locations = find_geolocation(neighborhoods_dict)\n","  university = neighborhood_locations['SWISU']\n","  distance_from_uni = {}\n","  for key in neighborhood_locations:\n","      distance_from_uni[key] = geodesic(neighborhood_locations[key], \n","                                        university).km\n","  return distance_from_uni\n","\n","def quality_size_mult(df, features):\n","  for paired_features in features:\n","    size_feature = paired_features[0]\n","    quality_feature = paired_features[1]\n","    df.loc[df[size_feature] > 0, size_feature] = (df[size_feature] * \n","                                                      df[quality_feature])\n","    \n","def size_from_total_divition(df, features):\n","  for paired_features in features:\n","    size_feature = paired_features[0]\n","    total_feature = paired_features[1]\n","    df.loc[df[total_feature] > 0, size_feature] = (df[size_feature]/\n","                                                        df[total_feature])\n","def rename_columns(df, names_dict):\n","  df.rename(columns= names_dict, inplace=True)\n","\n","def remove_features(df, columns):\n","  df_columns = df.columns\n","  for feature in columns:\n","      if feature in df_columns:\n","          df.drop(columns = [feature], inplace=True)\n","\n","def bsmt_bath(df):\n","  df[\"BsmtBaths\"] = (df[\"BsmtHalfBath\"]*0.5) + df[\"BsmtFullBath\"]\n","\n","def year_feature_by_max(df):\n","  years_features = ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'YrSold']\n","  max_year = df[years_features].max()\n","  df[years_features] = max_year - df[years_features]\n","\n","def target_encoding(df, features):\n","  group_by_avg = {}\n","  df_columns = df.columns\n","  for feature in features:\n","    feature_means = df.groupby(feature)['SalePrice'].mean()\n","    group_by_avg[feature] = feature_means\n","    df[feature + \"_by_avg\"] = df[feature].map(feature_means)\n","    df.drop(columns = [feature], inplace=True)\n","  return group_by_avg"],"metadata":{"id":"ch1R2BwzJPza"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def feature_engineering(df, neighborhoods_dict, qual_size_pairs,\n","                        size_total_pairs, remove_of_feature_engineering, \n","                        names_dict):\n","  quality_size_mult(df, qual_size_pairs)\n","  rename_columns(df, names_dict)\n","  size_from_total_divition(df, size_total_pairs)\n","  bsmt_bath(df)\n","  remove_features(df, remove_of_feature_engineering)\n","  year_feature_by_max(df)"],"metadata":{"id":"SWqp6i_ULLhe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["neighborhoods_dict = {'Blmngtn': 'Bloomington Heights, Ames, IA',\n","                      'Blueste': 'Bluestem, Ames, IA',\n","                      'BrDale': 'Briardale, Ames, IA',\n","                      'BrkSide': 'Brookside, Ames, IA',\n","                      'ClearCr': 'Clear Creek, Ames, IA',\n","                      'CollgCr': 'College Creek, Ames, IA',\n","                      'Crawfor': 'Crawford, Ames, IA',\n","                      'Edwards': 'Edwards, Ames, IA',\n","                      'Gilbert': 'Gilbert, IA',\n","                      'IDOTRR': 'Iowa Department of Transportation, Ames, IA',\n","                      'MeadowV': 'Meadow Place, Ames, IA',\n","                      'Mitchel': 'Mitchell, Ames, IA',\n","                      'NAmes': 'North Ames, Ames, IA',\n","                      'NoRidge': 'Northridge, Ames, IA',\n","                      'NPkVill': 'parkview, Ames, IA',\n","                      'NridgHt': 'Northridge Heights, Ames, IA',\n","                      'NWAmes': 'Northwest Ames, Ames, IA',\n","                      'OldTown': 'Old Town, Ames, IA',\n","                      'SWISU': 'Iowa State University, Ames, IA',\n","                      'Sawyer': 'Garfield Cir, Ames, IA',\n","                      'SawyerW': 'Illinois Ave, Ames, IA',\n","                      'Somerst': 'Somerset, Ames, IA',\n","                      'StoneBr': 'Stone Brooke, Ames, IA',\n","                      'Timber': 'Timberland, Ames, IA',\n","                      'Veenker': 'Veenker, Ames, IA'}\n","\n","qual_size_pairs = [('BsmtFinSF1', \"BsmtFinType1\"), ('BsmtFinSF2', \"BsmtFinSF2\"), \n","                   (\"TotalBsmtSF\",\"BsmtCond\"), (\"MasVnrArea\", \"MasVnr\"), \n","                   (\"GarageArea\", \"GarageCond\"), (\"Fireplaces\", \"FireplaceQu\")]\n","\n","size_total_pairs = [(\"BsmtFinSF1\", \"BsmtSizeQual\"), \n","                    (\"BsmtFinSF2\", \"BsmtSizeQual\"),\n","                    (\"BedroomAbvGr\", \"TotRmsAbvGrd\"), \n","                    (\"KitchenAbvGr\", \"TotRmsAbvGrd\")]\n","                    \n","remove_of_feature_engineering = [\"BsmtFinType1\", \"BsmtFinType2\", \"BsmtCond\",\n","                  \"MasVnr\", 'ExterCond', \"FireplaceQu\", \"BsmtHalfBath\",\n","                  \"BsmtFullBath\"]\n","\n","names_dict = {'TotalBsmtSF':'BsmtSizeQual', 'ExterQual': 'Exterior'}\n","\n","features_to_normalize = ['OverallQual', 'OverallCond', \"LotArea\", \"LotFrontage\",\n","                         'YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'YrSold', \n","                         'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', \n","                         '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', \n","                         'GarageArea', 'WoodDeckSF', 'OpenPorchSF', \n","                         'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n","                         'Neighborhood_by_avg', 'HouseStyle_by_avg', \n","                         'BsmtSizeQual','RoofStyle_by_avg', 'SaleType_by_avg',\n","                         'Foundation_merged_by_avg', 'Exterior_combined_merged_by_avg']\n","\n","target_encoding_features = ['Neighborhood', 'HouseStyle', 'RoofStyle', 'SaleType', \n","                            'Foundation_merged', 'Exterior_combined_merged']"],"metadata":{"id":"JqiQ5OdnLn7W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#train fit\n","train = pd.read_csv('train.csv')\n","\n","distance_from_uni = find_distance_from_university(neighborhoods_dict)\n","train['dist_from_uni'] = train.Neighborhood.map(distance_from_uni)\n","\n","processed_train = preprocessing(train, continues_features, category_ordered, \n","                                remove_features_list, merged_features_dict, \n","                                unicode_features_dict, onehot_columns)\n","\n","target_encoding_dict = target_encoding(processed_train, target_encoding_features)\n","\n","feature_engineering(processed_train, neighborhoods_dict, \n","                                       qual_size_pairs, size_total_pairs, \n","                                       remove_of_feature_engineering, names_dict)\n","\n","y_train = processed_train['SalePrice']\n","y_train = np.log1p(y_train)\n","X_train = processed_train.copy().loc[:, processed_train.columns != 'SalePrice']\n","\n","scaler = MinMaxScaler()\n","scaler.fit(X_train[features_to_normalize])\n","X_train[features_to_normalize] = scaler.transform(X_train[features_to_normalize])"],"metadata":{"id":"cZe3z_APLbtG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#basic model score\n","clf = LinearRegression()\n","scores = cross_val_score(clf, X_train, y_train, cv=5)\n","print(scores)\n","print(np.mean(scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SAO7xJVbSGCq","executionInfo":{"status":"ok","timestamp":1643536061479,"user_tz":-120,"elapsed":24,"user":{"displayName":"Noy Nissim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03201112091562042773"}},"outputId":"eb700367-31f7-4d72-fe2e-4cb6a3ae1086"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.89888269 0.88190907 0.90615813 0.89892889 0.80120257]\n","0.8774162697059292\n"]}]},{"cell_type":"markdown","source":["# Models evaluation\n","\n","We check some models long the way. To find the best hyper parameters we used gridSearchCV. The models we tested:\n","- Ridge regression\n","- XGboost\n","- Random forest\n","- Gradient boost"],"metadata":{"id":"rjmYovbxeMZJ"}},{"cell_type":"code","source":["clf = Ridge(random_state=0)\n","scores = cross_val_score(clf, X_train, y_train, cv=5)\n","print(scores)\n","print(np.mean(scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LhbQtt2Dh_9a","executionInfo":{"status":"ok","timestamp":1643486266548,"user_tz":-120,"elapsed":12,"user":{"displayName":"Harel Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwJ3OCVyJY65jKLzMqY5SMvTdvTdICLKM2HMDchA=s64","userId":"02907729866407537485"}},"outputId":"eabaa0b1-cf7c-443e-fc25-9a3a4a28f0df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.90187645 0.8843094  0.90340694 0.89791707 0.80950845]\n","0.8794036631740383\n"]}]},{"cell_type":"code","source":["\n","clf = RandomForestRegressor(random_state=0)\n","scores = cross_val_score(clf, X_train, y_train, cv=5)\n","print(scores)\n","print(np.mean(scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h_DH4GqUmiaG","executionInfo":{"status":"ok","timestamp":1643486275504,"user_tz":-120,"elapsed":8961,"user":{"displayName":"Harel Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwJ3OCVyJY65jKLzMqY5SMvTdvTdICLKM2HMDchA=s64","userId":"02907729866407537485"}},"outputId":"5e5902f4-383f-473d-a52c-e7caa9f61967"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.88315445 0.87105533 0.88000267 0.8803987  0.8536289 ]\n","0.8736480091522525\n"]}]},{"cell_type":"code","source":["\n","clf = XGBRegressor(objective ='reg:squarederror', random_state=0)\n","scores = cross_val_score(clf, X_train, y_train, cv=5)\n","print(scores)\n","print(np.mean(scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vFoDTp3kkGds","executionInfo":{"status":"ok","timestamp":1643486276891,"user_tz":-120,"elapsed":1392,"user":{"displayName":"Harel Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwJ3OCVyJY65jKLzMqY5SMvTdvTdICLKM2HMDchA=s64","userId":"02907729866407537485"}},"outputId":"e9777d24-ab31-4002-fdf4-3cc7acc9aa0c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.91534426 0.89607287 0.8981973  0.9011251  0.884733  ]\n","0.8990945051697059\n"]}]},{"cell_type":"code","source":["\n","clf = GradientBoostingRegressor(random_state=0)\n","scores = cross_val_score(clf, X_train, y_train, cv=5)\n","print(scores)\n","print(np.mean(scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"51FhX7Mxmz2f","executionInfo":{"status":"ok","timestamp":1643486280018,"user_tz":-120,"elapsed":3131,"user":{"displayName":"Harel Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwJ3OCVyJY65jKLzMqY5SMvTdvTdICLKM2HMDchA=s64","userId":"02907729866407537485"}},"outputId":"fa2c4d9f-d6cd-4ec3-9c40-3e81d07a899e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.91303509 0.88609917 0.89770736 0.9067837  0.88757866]\n","0.8982407957581428\n"]}]},{"cell_type":"markdown","source":["# Improve models\n","We saw that the gradient boosting and the XGBoost are our best models and close with thier scores, so we decided to try to improve them both."],"metadata":{"id":"ARAUCtx9Vvzv"}},{"cell_type":"code","source":["#train fit\n","train = pd.read_csv('train.csv')\n","\n","distance_from_uni = find_distance_from_university(neighborhoods_dict)\n","train['dist_from_uni'] = train.Neighborhood.map(distance_from_uni)\n","\n","processed_train = preprocessing(train, continues_features, category_ordered, \n","                                remove_features_list, merged_features_dict, \n","                                unicode_features_dict, onehot_columns)\n","\n","target_encoding_dict = target_encoding(processed_train, target_encoding_features)\n","\n","feature_engineering(processed_train, neighborhoods_dict, \n","                                       qual_size_pairs, size_total_pairs, \n","                                       remove_of_feature_engineering, names_dict)\n","\n","y_train = processed_train['SalePrice']\n","y_train = np.log1p(y_train)\n","X_train = processed_train.copy().loc[:, processed_train.columns != 'SalePrice']\n","\n","scaler = MinMaxScaler()\n","scaler.fit(X_train[features_to_normalize])\n","X_train[features_to_normalize] = scaler.transform(X_train[features_to_normalize])"],"metadata":{"id":"PuHt8S2TZf6g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#test fit\n","test = pd.read_csv('test.csv')\n","\n","test['dist_from_uni'] = test.Neighborhood.map(distance_from_uni)\n","\n","X_test = preprocessing(test, continues_features, category_ordered, \n","                                remove_features_list, merged_features_dict, \n","                                unicode_features_dict, onehot_columns)\n","\n","for feature, feature_values in target_encoding_dict.items():\n","    X_test[feature + \"_by_avg\"] = X_test[feature].map(feature_values)\n","    X_test.drop(columns = [feature], inplace=True)\n","\n","feature_engineering(X_test, neighborhoods_dict, qual_size_pairs, \n","                    size_total_pairs, remove_of_feature_engineering, names_dict)\n","\n","X_test[features_to_normalize] = scaler.transform(X_test[features_to_normalize])"],"metadata":{"id":"hZQgkwfrt6W5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Multiply new features \n","- We noticed that there are some feature engineering that contribute to those models specific so we decided to add them and normalize:\n"," \n"," $Lot = LotArea \\cdot LotFrontage$\n","\n"," $Pool = PoolArea \\cdot PoolQC$\n","\n"," $Overall = OverallQual \\cdot OverallCond$"],"metadata":{"id":"kfBeUJvOiE7o"}},{"cell_type":"code","source":["def multiply_new_features(df):\n","  df[\"Lot\"] = (df[\"LotArea\"] * df[\"LotFrontage\"])\n","  df[\"Pool\"] = (df[\"PoolArea\"] * df[\"PoolQC\"])\n","  df[\"Overall\"] = (df[\"OverallQual\"] * df[\"OverallCond\"])\n","  df.drop(columns = [\"LotArea\", \"LotFrontage\", \"PoolArea\", \"PoolQC\", \n","                        \"OverallQual\", \"OverallCond\"], inplace=True)"],"metadata":{"id":"PaQgbXL4xc1a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["multiply_new_features(X_train)\n","\n","scaler2 = MinMaxScaler()\n","normalize_list = [\"Overall\", \"Lot\", \"Pool\"]\n","scaler2.fit(X_train[normalize_list])\n","X_train[normalize_list] = scaler2.transform(X_train[normalize_list])\n","\n","multiply_new_features(X_test)\n","X_test[normalize_list] = scaler2.transform(X_test[normalize_list])"],"metadata":{"id":"ZrhDglZdWuYT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Drop more features\n","We decided to drop Utilities feature because it's gives no added information about the test data as all the test data has the same utility."],"metadata":{"id":"WcDv8q-OjVgb"}},{"cell_type":"code","source":["X_train.drop(columns=['Utilities', 'MSSubClass'], inplace=True)\n","X_test.drop(columns=[ 'Utilities', 'MSSubClass'], inplace=True)"],"metadata":{"id":"o_F21D2XZnTG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# GridSearch - Note: This takes a long time to run!\n","\n","In order to tune the hyperparameters of the models we chose, we decided to go with sklearn's GridSearchCV.\n","Of course, there are a lot of hyperparameters but we decided to focus on some of them:\n","\n","* max_depth\n","* n_estimators\n","* learning_rate\n","* colsample_bytree / max_features\n","* subsample\n","\n","We decided to first do a rough grid search and then fine-tune the search near the best values."],"metadata":{"id":"sWdagMuzgz57"}},{"cell_type":"code","source":["params_xgb = {'max_depth': [3,6,10],\n","                   'n_estimators': [100, 500, 1000],\n","                   'learning_rate': [0.01, 0.05, 0.1],\n","                   'colsample_bytree': [0.3, 0.7, 1.0],\n","              'subsample': [0.5, 0.7, 1.0]}\n","\n","params_gb = {'max_depth': [3,6,10],\n","                   'n_estimators': [100, 500, 1000],\n","                   'learning_rate': [0.01, 0.05, 0.1],\n","                   'max_features': [0.3, 0.7, 1.0],\n","             'subsample': [0.5, 0.7, 1.0]}\n"],"metadata":{"id":"yOyKhFhzV8Q-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["xgb = XGBRegressor(objective ='reg:squarederror', random_state=0)\n","gb = GradientBoostingRegressor(random_state=0)\n","\n","clf_gb = GridSearchCV(estimator=gb, param_grid=params_gb)\n","clf_xgb = GridSearchCV(estimator=xgb, param_grid=params_xgb, scoring='neg_mean_squared_error')\n","\n","clf_xgb.fit(X_train, y_train)\n","clf_gb.fit(X_train, y_train)\n","\n","print('Best parameters for XGBoost: ', clf_xgb.best_params_)\n","print('Best parameters for GradientBoosting: ', clf_gb.best_params_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WcBYouLYauAy","executionInfo":{"status":"ok","timestamp":1643493801987,"user_tz":-120,"elapsed":5957022,"user":{"displayName":"Harel Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwJ3OCVyJY65jKLzMqY5SMvTdvTdICLKM2HMDchA=s64","userId":"02907729866407537485"}},"outputId":"214c0fcb-3136-49df-d835-8a3ab1aae798"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Best parameters for XGBoost:  {'colsample_bytree': 0.3, 'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 500, 'subsample': 0.7}\n","Best parameters for GradientBoosting:  {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 0.3, 'n_estimators': 1000, 'subsample': 0.5}\n"]}]},{"cell_type":"markdown","source":["## Fine-tuning the parameters"],"metadata":{"id":"Ap7QEKsVcB8d"}},{"cell_type":"code","source":["params_xgb = {'max_depth': [2,3,4],\n","                   'n_estimators': [400, 500, 600],\n","                   'learning_rate': [0.04, 0.05, 0.6],\n","                   'colsample_bytree': [0.2, 0.3, 0.4],\n","              'subsample': [0.6, 0.7, 0.8]}\n","\n","params_gb = {'max_depth': [5,6,7],\n","                   'n_estimators': [900, 1000, 1100],\n","                   'learning_rate': [0.05, 0.1, 0.15],\n","                   'max_features': [0.2, 0.3, 0.4],\n","             'subsample': [0.4, 0.5, 0.6]}\n"],"metadata":{"id":"UaWgqy7RcG1K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["xgb = XGBRegressor(objective ='reg:squarederror', random_state=0)\n","gb = GradientBoostingRegressor(random_state=0)\n","\n","clf_gb = GridSearchCV(estimator=gb, param_grid=params_gb)\n","clf_xgb = GridSearchCV(estimator=xgb, param_grid=params_xgb, scoring='neg_mean_squared_error')\n","\n","clf_xgb.fit(X_train, y_train)\n","clf_gb.fit(X_train, y_train)\n","\n","print('Best parameters for XGBoost: ', clf_xgb.best_params_)\n","print('Best parameters for GradientBoosting: ', clf_gb.best_params_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vIoPDnk3cHpR","executionInfo":{"status":"ok","timestamp":1643497632312,"user_tz":-120,"elapsed":3564069,"user":{"displayName":"Harel Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwJ3OCVyJY65jKLzMqY5SMvTdvTdICLKM2HMDchA=s64","userId":"02907729866407537485"}},"outputId":"3525d8ed-1470-4786-9214-46c429e67429"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Best parameters for XGBoost:  {'colsample_bytree': 0.3, 'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 500, 'subsample': 0.6}\n","Best parameters for GradientBoosting:  {'learning_rate': 0.05, 'max_depth': 5, 'max_features': 0.3, 'n_estimators': 900, 'subsample': 0.6}\n"]}]},{"cell_type":"markdown","source":["So these are the final hyperparameters for our models\n","\n","Best parameters for XGBoost:  {'colsample_bytree': 0.3, 'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 500, 'subsample': 0.6}\n","\n","\n","Best parameters for GradientBoosting:  {'learning_rate': 0.05, 'max_depth': 5, 'max_features': 0.3, 'n_estimators': 900, 'subsample': 0.6}"],"metadata":{"id":"fy84f2_bKNnM"}},{"cell_type":"code","source":["clf = XGBRegressor(objective ='reg:squarederror', random_state=0, \n","                   colsample_bytree= 0.3, learning_rate= 0.05, \n","                   max_depth= 4, n_estimators= 500, subsample= 0.6)\n","scores = cross_val_score(clf, X_train, y_train, cv=5)\n","print(scores)\n","print(np.mean(scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T_NkiFeSZnGF","executionInfo":{"status":"ok","timestamp":1643536268333,"user_tz":-120,"elapsed":4809,"user":{"displayName":"Noy Nissim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03201112091562042773"}},"outputId":"b87725e9-9383-4df0-cbd4-1d9b0adb2b48"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.91939999 0.89756753 0.91275871 0.92082549 0.91456167]\n","0.9130226764471491\n"]}]},{"cell_type":"code","source":["clf = GradientBoostingRegressor(random_state=0, learning_rate= 0.05, \n","                                max_depth= 5, max_features= 0.3, \n","                                n_estimators= 900, subsample= 0.6)\n","scores = cross_val_score(clf, X_train, y_train, cv=5)\n","print(scores)\n","print(np.mean(scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0I3SKPnJZpHV","executionInfo":{"status":"ok","timestamp":1643577514009,"user_tz":-120,"elapsed":19493,"user":{"displayName":"Noy Nissim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03201112091562042773"}},"outputId":"0bcd249c-0558-472a-fbca-9f6c45a9fcde"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.9223826  0.90103631 0.90207787 0.92086175 0.91563298]\n","0.9123983009838776\n"]}]},{"cell_type":"markdown","source":["# Final model predictions- stack model\n","\n","After the grid we saw that we get almost the same results with XGBoost and Gradient Boosting. We decided to use our models with stack ensemble method, we did a basic fine-tuning to each model which is not our main model, and used our results from the grid search. \n","\n","The models for stacking:"],"metadata":{"id":"gd6RxeF9cJFm"}},{"cell_type":"code","source":["from sklearn.ensemble import StackingRegressor\n","\n","estimators = [('lr', LinearRegression(n_jobs = -1)),\n","              ('rd', Ridge(random_state=0, alpha = 4.84)),\n","              ('rf', RandomForestRegressor(random_state=0)),\n","              ('gb', GradientBoostingRegressor(random_state=0, learning_rate= 0.05, \n","                               max_depth= 5, max_features= 0.3,\n","                               n_estimators= 900, subsample= 0.6)),\n","             ('xgb', XGBRegressor(objective ='reg:squarederror', random_state=0, \n","                   colsample_bytree= 0.3, learning_rate= 0.05, \n","                   max_depth= 4, n_estimators= 500, subsample= 0.6))]\n","\n","model = StackingRegressor(estimators=estimators, final_estimator=Ridge(random_state=0, alpha = 4.84), cv=5)\n","model.fit(X_train, y_train)\n","score = model.score(X_train, y_train)\n","print(score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8y-TiIhw-5pu","executionInfo":{"status":"ok","timestamp":1643579806811,"user_tz":-120,"elapsed":29351,"user":{"displayName":"Noy Nissim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03201112091562042773"}},"outputId":"10d0a727-c802-4e47-c8d4-4e154135e6fb"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9875979387935075\n"]}]},{"cell_type":"markdown","source":["The score we got shows us that we are overfitting so we decided to remove:\n","\n","- Random forest- because we learned that it's tend to overfitting\n","- XGBoost- because we suspec that with the gradient boosting they contribute to the overfitting of our model so we decided to stay with one of them\n","- On gradient boost we used the default params to avoid from overfitting."],"metadata":{"id":"Pi7ku06F4dod"}},{"cell_type":"code","source":["estimators = [('lr', LinearRegression(n_jobs = -1)),\n","              ('rd', Ridge(random_state=0, alpha = 4.84)),\n","              ('gb', GradientBoostingRegressor(random_state=0))]\n","\n","model = StackingRegressor(estimators=estimators, final_estimator=Ridge(random_state=0, alpha = 4.84), cv=5)\n","model.fit(X_train, y_train)\n","score = model.score(X_train, y_train)\n","print(score)"],"metadata":{"id":"nl_aETmt54WN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643579810720,"user_tz":-120,"elapsed":3923,"user":{"displayName":"Noy Nissim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03201112091562042773"}},"outputId":"f3ffa3f2-3c5e-419f-d3e7-7b81a6ecb78b"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9435115209382899\n"]}]},{"cell_type":"code","source":["y_pred = model.predict(X_test)\n","y_pred_final = np.expm1(y_pred)\n","submission = pd.DataFrame({'Id':test.Id, 'Predicted': y_pred_final})\n","submission.to_csv('submission.csv', index=False)"],"metadata":{"id":"XpWbETQfuk_a","executionInfo":{"status":"ok","timestamp":1643579810721,"user_tz":-120,"elapsed":6,"user":{"displayName":"Noy Nissim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03201112091562042773"}}},"execution_count":42,"outputs":[]}]}